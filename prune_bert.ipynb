{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prune_bert",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ylzp_5lwkuy"
      },
      "source": [
        "!pip install -qq transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5ArtkAmw2TX",
        "outputId": "5182bf45-bda7-4600-fd76-1b439765eb2e"
      },
      "source": [
        "!git clone https://huggingface.co/huggingface/prunebert-base-uncased-6-finepruned-w-distil-squad"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'prunebert-base-uncased-6-finepruned-w-distil-squad'...\n",
            "remote: Enumerating objects: 24, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 24 (delta 7), reused 0 (delta 0)\u001b[K\n",
            "Unpacking objects: 100% (24/24), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acVTvRj-wdFs"
      },
      "source": [
        "import transformers\n",
        "\n",
        "#max number of tokens in the sentence\n",
        "MAX_LEN = 512\n",
        "\n",
        "#batch sizes is small cause model is huge\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "VALID_BATCH_SIZE = 4\n",
        "\n",
        "#lets train for a maximum of 10 epochs\n",
        "EPOCHS = 10\n",
        "BERT_PATH = 'prunebert-base-uncased-6-finepruned-w-distil-squad/'\n",
        "#save model path\n",
        "MODEL_PATH ='prune_model.bin'\n",
        "#training file\n",
        "TRAINING_FILE = 'augment_daily.csv'\n",
        "\n",
        "#define the tokenizer\n",
        "#we use tokenizer and model\n",
        "#from hugging face transformers\n",
        "TOKENIZER = transformers.BertTokenizer.from_pretrained(\n",
        "    BERT_PATH,\n",
        "    do_lower_case=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AibQAxnwgTg"
      },
      "source": [
        "#dataset.py\n",
        "import torch\n",
        "\n",
        "class BertDataset:\n",
        "  def __init__(self,log,target):\n",
        "    self.log = log\n",
        "    self.target = target\n",
        "    self.tokenizer = TOKENIZER\n",
        "    self.max_len = MAX_LEN\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.log)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    log = str(self.log[item])\n",
        "    log = \" \".join(log.split())\n",
        "\n",
        "    # enocode_plus comes from hugging face transformers\n",
        "    # and exists for all tokenizers they offer\n",
        "    # it can be used to convert a given string\n",
        "    # to ids and mask and token type ids which\n",
        "    # are used for model like BERT\n",
        "    # review should be a string\n",
        "    inputs = self.tokenizer.encode_plus(\n",
        "        log,\n",
        "        None,\n",
        "        add_special_tokens=True,\n",
        "        max_length=self.max_len,\n",
        "        pad_to_max_length=True,\n",
        "    ) \n",
        "    #ids are ids of tokens generated\n",
        "    ids = inputs[\"input_ids\"]\n",
        "    #mask is 1 where we have input and 0 where we have padding\n",
        "    mask = inputs['attention_mask']\n",
        "    token_type_ids = inputs['token_type_ids']\n",
        "\n",
        "    return {\n",
        "        \"ids\": torch.tensor(ids, dtype=torch.long),\n",
        "        \"mask\": torch.tensor(mask,dtype=torch.long),\n",
        "        \"token_type_ids\": torch.tensor(token_type_ids,dtype=torch.long),\n",
        "        \"targets\": torch.tensor(self.target[item],dtype=torch.long)\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGWfGTPSxCZa"
      },
      "source": [
        "#model.py\n",
        "import transformers\n",
        "from transformers import BertForSequenceClassification\n",
        "import torch.nn as nn\n",
        "\n",
        "class BERTBaseUncased(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(BERTBaseUncased, self).__init__()\n",
        "    #fetch the model from BERT_PATH \n",
        "    self.bert = BertForSequenceClassification.from_pretrained(\n",
        "    \"huggingface/prunebert-base-uncased-6-finepruned-w-distil-squad\",\n",
        "    num_labels = 5,   \n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        "    )\n",
        "\n",
        "  def forward(self, ids, mask, token_type_ids):\n",
        "    #bert default settings returns\n",
        "    #last hidden state and output of the bert pooler layer\n",
        "    #output of the pooler which of size (batch_size,hidden_size)\n",
        "    #hidden size can be 768(Bert base) or 1024(Bert Large)\n",
        "    output = self.bert(\n",
        "        ids,\n",
        "        attention_mask=mask,\n",
        "        token_type_ids=token_type_ids\n",
        "    )\n",
        "\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ2pK9qKxZeI"
      },
      "source": [
        "#engine.py\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "\n",
        "def loss_fn(outputs, targets):\n",
        "  \"\"\"\n",
        "  function that returns\n",
        "  \"\"\"\n",
        "  return nn.CrossEntropyLoss()(outputs,targets)\n",
        "\n",
        "def train_fn(data_loader,model,optimizer,device,scheduler):\n",
        "  \"\"\"\n",
        "  Train the model from one epoch\n",
        "  :param data_loader: torch dataloader\n",
        "  :param model:bert base model\n",
        "  :param optimizer: adam, sgd..etc\n",
        "  :param device: can be cpu or gpu\n",
        "  :param scheduler: learning rate scheduler\n",
        "  \"\"\"\n",
        "  model.train()\n",
        "  #loop over all batches\n",
        "  i = 0\n",
        "  size = len(data_loader)\n",
        "  for d in tqdm(data_loader,total=size):\n",
        "    #extract ids, token type ids and mask\n",
        "    ids = d['ids']\n",
        "    token_type_ids = d['token_type_ids']\n",
        "    mask = d['mask']\n",
        "    targets = d[\"targets\"]\n",
        "\n",
        "    #move everything to device\n",
        "    ids = ids.to(device, dtype=torch.long)\n",
        "    token_type_ids = token_type_ids.to(device,dtype=torch.long)\n",
        "    mask = mask.to(device,dtype=torch.long)\n",
        "    targets = targets.to(device, torch.long)\n",
        "\n",
        "    #zero-grad optimizers\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(\n",
        "        ids=ids,\n",
        "        mask=mask,\n",
        "        token_type_ids=token_type_ids\n",
        "    )[0]\n",
        "    loss = loss_fn(outputs, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "def eval_fn(data_loader,model,device):\n",
        "  model.eval()\n",
        "  fin_targets = []\n",
        "  fin_outputs = []\n",
        "  #to not run out of GPU & not to change gradients\n",
        "  with torch.no_grad(): \n",
        "    for d in data_loader:\n",
        "      ids = d[\"ids\"]\n",
        "      token_type_ids = d[\"token_type_ids\"]\n",
        "      mask = d[\"mask\"]\n",
        "      targets = d[\"targets\"]\n",
        "\n",
        "      ids = ids.to(device, dtype=torch.long)\n",
        "      token_type_ids = token_type_ids.to(device,dtype=torch.long)\n",
        "      mask = mask.to(device,dtype=torch.long)\n",
        "      targets = targets.to(device, torch.long)\n",
        "\n",
        "      outputs = model(\n",
        "          ids,\n",
        "          mask=mask,\n",
        "          token_type_ids=token_type_ids,\n",
        "      )[0]\n",
        "      val_loss = loss_fn(outputs, targets)\n",
        "      targets = targets.cpu().detach()\n",
        "      fin_targets.extend(targets.numpy().tolist())\n",
        "      outputs = outputs.cpu().detach()\n",
        "      fin_outputs.extend(outputs.numpy().tolist())\n",
        "\n",
        "    return val_loss, fin_outputs, fin_targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iupy6o82xiMV"
      },
      "source": [
        "#train.py\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import torch.nn as nn \n",
        "\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train():\n",
        "  dfx = pd.read_csv(TRAINING_FILE).fillna('none')\n",
        "  dfx = dfx.sample(frac=1).reset_index(drop=True)\n",
        "  # dfx = dfx.iloc[:100,:]\n",
        "  dfx_mapper = {\n",
        "      'food':0, 'transport':1, 'shopping':2, 'bills':3, 'credit':4\n",
        "  }\n",
        "  dfx.cat = dfx.cat.map(dfx_mapper)\n",
        "  df_train,df_valid = model_selection.train_test_split(\n",
        "      dfx,\n",
        "      test_size=0.1,\n",
        "      random_state=42,\n",
        "      stratify=dfx.cat.values\n",
        "  )\n",
        "\n",
        "  df_train = df_train.reset_index(drop=True)\n",
        "  df_valid = df_valid.reset_index(drop=True)\n",
        "\n",
        "  train_dataset = BertDataset(\n",
        "      log=df_train.logs.values,\n",
        "      target=df_train.cat.values\n",
        "  )\n",
        "\n",
        "  train_dataloader = torch.utils.data.DataLoader(\n",
        "      train_dataset,\n",
        "      batch_size=TRAIN_BATCH_SIZE,\n",
        "      num_workers=3\n",
        "  )\n",
        "\n",
        "  valid_dataset = BertDataset(\n",
        "      log=df_valid.logs.values,\n",
        "      target=df_valid.cat.values\n",
        "  )\n",
        "\n",
        "  valid_dataloader = torch.utils.data.DataLoader(\n",
        "      valid_dataset,\n",
        "      batch_size=VALID_BATCH_SIZE,\n",
        "      num_workers=1\n",
        "  )\n",
        "\n",
        "  device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "  model = BERTBaseUncased()\n",
        "  model.to(device)\n",
        "           \n",
        "  #create parameters we want to optimize\n",
        "  #we generally dont use any decay for bias\n",
        "  #and weight layers\n",
        "  param_optimizer = list(model.named_parameters())\n",
        "  no_decay = [\"bias\",\"LayerNorm.bias\",\"LayerNorm.weight\"]\n",
        "  optimizer_parameters = [\n",
        "               {\n",
        "                   \"params\":[\n",
        "                             p for n,p in param_optimizer if \n",
        "                             not any(nd in n for nd in no_decay)\n",
        "                   ],\n",
        "                   \"weight_decay\": 0.001,\n",
        "               },\n",
        "               {\n",
        "                   \"params\":[\n",
        "                             p for n,p in param_optimizer if \n",
        "                             any(nd in n for nd in no_decay)\n",
        "                   ],\n",
        "                   \"weight_decay\": 0.0,\n",
        "               },\n",
        "  ]\n",
        "\n",
        "  #calculate the number of training steps\n",
        "  #used by schduler\n",
        "  num_train_steps = int(\n",
        "      len(df_train) / TRAIN_BATCH_SIZE * EPOCHS\n",
        "  )\n",
        "\n",
        "  #AdamW in widely used optimizer for transformer based model\n",
        "  optimizer = AdamW(optimizer_parameters, lr =3e-5)\n",
        "\n",
        "  #fetch the schduler\n",
        "  scheduler = get_linear_schedule_with_warmup(\n",
        "      optimizer,\n",
        "      num_warmup_steps=0,\n",
        "      num_training_steps=num_train_steps\n",
        "  )\n",
        "\n",
        "  # model = nn.DataParallel(model)\n",
        "\n",
        "  best_accuracy = 0\n",
        "  for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch: {epoch}\")\n",
        "    print(\"Model training..\")\n",
        "    print(len(train_dataloader))\n",
        "    train_fn(train_dataloader, model,\n",
        "             optimizer,device,scheduler)\n",
        "    print(\"Model Evalutaion..\")\n",
        "    val_loss, outputs, targets = eval_fn(valid_dataloader,model,device)\n",
        "\n",
        "    accuracy = metrics.accuracy_score(targets, np.argmax(outputs,axis=1))\n",
        "    print(f\"epoch: {epoch}, val acc: {accuracy}, val_loss: {val_loss}\")\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "      torch.save(model.state_dict(),MODEL_PATH)\n",
        "      best_accuracy = accuracy\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NvzqqwjVxlj5",
        "outputId": "d35f98bd-052a-4837-d3b4-cea5e8602fae"
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at huggingface/prunebert-base-uncased-6-finepruned-w-distil-squad were not used when initializing BertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huggingface/prunebert-base-uncased-6-finepruned-w-distil-squad and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "  0%|          | 0/837 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Model training..\n",
            "837\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100%|██████████| 837/837 [11:04<00:00,  1.26it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Evalutaion..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, val acc: 0.9623655913978495, val_loss: 0.008109736256301403\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/837 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Model training..\n",
            "837\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100%|██████████| 837/837 [11:04<00:00,  1.26it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Evalutaion..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 1, val acc: 0.978494623655914, val_loss: 0.00481443339958787\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/837 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 2\n",
            "Model training..\n",
            "837\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100%|██████████| 837/837 [11:04<00:00,  1.26it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Evalutaion..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "  0%|          | 0/837 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 2, val acc: 0.9771505376344086, val_loss: 0.0033930083736777306\n",
            "Epoch: 3\n",
            "Model training..\n",
            "837\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100%|██████████| 837/837 [11:03<00:00,  1.26it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Evalutaion..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "  0%|          | 0/837 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 3, val acc: 0.9758064516129032, val_loss: 0.002098868601024151\n",
            "Epoch: 4\n",
            "Model training..\n",
            "837\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100%|██████████| 837/837 [11:04<00:00,  1.26it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Evalutaion..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "  0%|          | 0/837 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 4, val acc: 0.9758064516129032, val_loss: 0.0017521880799904466\n",
            "Epoch: 5\n",
            "Model training..\n",
            "837\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            " 15%|█▍        | 122/837 [01:37<09:27,  1.26it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-9f9809073c39>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     train_fn(train_dataloader, model,\n\u001b[0;32m--> 102\u001b[0;31m              optimizer,device,scheduler)\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model Evalutaion..\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-868ce93bb02c>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(data_loader, model, optimizer, device, scheduler)\u001b[0m\n\u001b[1;32m     43\u001b[0m     )[0]\n\u001b[1;32m     44\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM4qfbAZ3dWE"
      },
      "source": [
        "def sentence_prediction(sentence):\n",
        "  tokenizer = TOKENIZER\n",
        "  max_len = MAX_LEN\n",
        "\n",
        "  log = str(sentence)\n",
        "  log = \" \".join(log.split())\n",
        "  inputs = tokenizer.encode_plus(\n",
        "      log,\n",
        "      None,\n",
        "      add_special_tokens=True,\n",
        "      max_length=max_len\n",
        "  )\n",
        "\n",
        "  ids = inputs[\"input_ids\"]\n",
        "  mask = inputs['attention_mask']\n",
        "  token_type_ids = inputs['token_type_ids']\n",
        "\n",
        "  #add padding\n",
        "  padding_length = max_len - len(ids)\n",
        "  ids = ids + ([0] * padding_length)\n",
        "  mask = mask + ([0] * padding_length)\n",
        "  token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cjc7eDxHwHa"
      },
      "source": [
        "sentence = \"i have purchased a new shoe\"\n",
        "tokenizer = TOKENIZER\n",
        "max_len = MAX_LEN\n",
        "\n",
        "log = str(sentence)\n",
        "log = \" \".join(log.split())\n",
        "inputs = tokenizer.encode_plus(\n",
        "      log,\n",
        "      None,\n",
        "      add_special_tokens=True,\n",
        "      max_length=max_len\n",
        "  )\n",
        "\n",
        "ids = inputs[\"input_ids\"]\n",
        "mask = inputs['attention_mask']\n",
        "token_type_ids = inputs['token_type_ids']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrQBKBVlH5B4"
      },
      "source": [
        "  #add padding\n",
        "  padding_length = max_len - len(ids)\n",
        "  ids = ids + ([0] * padding_length)\n",
        "  mask = mask + ([0] * padding_length)\n",
        "  token_type_ids = token_type_ids + ([0] * padding_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r82ydRk8Vbbu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}